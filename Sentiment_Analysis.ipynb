{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONPmquP3GW7UTD3xKTjzi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasnikita/Sentiment-Analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItVK-ohCf0lu"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import os \n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import preprocessing\n",
        "from bs4 import BeautifulSoup\n",
        "import re \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegressionCV \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "import tensorflow as tf \n",
        "import joblib\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Knm6hx5Wf-AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading our Training Dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Train.csv')"
      ],
      "metadata": {
        "id": "Yg2UAX-kgFLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "jgasBJIYggNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape()"
      ],
      "metadata": {
        "id": "w8z9YyJXgjob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "knoh4272glZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We intend to clean the text by\n",
        "1) removing stopwords\n",
        "2) Dealing with auxillaries by seperating 'nots'\n",
        "3) Removing HTML tags (bs4 library is used for this)\n",
        "4) Removing special charecters and digits\n",
        "5) Converting the words into lower case"
      ],
      "metadata": {
        "id": "xXek-8zzgqQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords  like not,nor etc. which could influence the sentiment label, were removed from the stopwords list.\n",
        "stopword= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
        "#dealing with auxialaries\n",
        "def seperate_nots(text):\n",
        "  text = re.sub(\"can't\",'can not',text) \n",
        "  text = re.sub(\"shan't\",'shall not',text)\n",
        "  text = re.sub(\"won't\",'will not',text)\n",
        "  text = re.sub(\"n't\",' not',text) \n",
        "  text = re.sub(\"'re\",' are',text)\n",
        "  text = re.sub(\"'ve\",' have',text) \n",
        "  text = re.sub(\"'m\",' am',text)\n",
        "  text = re.sub(\"'ll\",' will',text)\n",
        "  text = re.sub(\"'d\",' would',text)\n",
        "  text = re.sub(\"'s\",' is',text) \n",
        "  return text\n",
        "\n",
        "def preprocessing_text(text,stopword):\n",
        "  lst = []\n",
        "  text = BeautifulSoup(text, \"lxml\").text #code for removing html tags \n",
        "  text = re.sub('[^a-z A-Z0-9]+','',text) #removing special characters \n",
        "  text = re.sub('(\\S*\\d\\S*)','',text).strip() #removing numbers/digits\n",
        "  text = re.sub('\"http:\\S+\"', '',text) \n",
        "  text = seperate_nots(text)\n",
        "\n",
        "  words = [word for word in text.split() if word.lower() not in stopword]\n",
        "  new_text = \" \".join(words)\n",
        "  print(new_text)\n",
        "  print(\"Old length: \", len(text))\n",
        "  print(\"New length: \", len(new_text))\n",
        "\n",
        " # for word in text.split():\n",
        "  #  if word not in stopword:\n",
        "   #   l = word.lower() \n",
        "    #  lst.append(l) \n",
        "     # x = ' '.join(lst)\n",
        " # return x"
      ],
      "metadata": {
        "id": "F_nt1Acngnl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"a great taste\""
      ],
      "metadata": {
        "id": "NRsuYyYJgybt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "preprocessing_text(sentence,stopword)"
      ],
      "metadata": {
        "id": "KzhloCQxgzXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c901d5f8-8b87-4897-c44b-4ef39169cd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great taste\n",
            "Old length:  13\n",
            "New length:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].apply(lambda x: preprocessing_text(x,stopword))"
      ],
      "metadata": {
        "id": "YlqyKT_5g1JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('clean_data.csv',index=False) #saving the clean data"
      ],
      "metadata": {
        "id": "TOuUT8X3g3uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/clean_data.csv')"
      ],
      "metadata": {
        "id": "e_FOUIudg6qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "_5Y8At8IhByS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "UcozrfmkhEZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(review for review in data.text)\n",
        "print (\"There are {} words in the combination of all review.\".format(len(text))) \n",
        "plt.figure(figsize=(10,7))\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopword,background_color=\"white\").generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFntDPpLhHtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pos = data[data['label'] == 1]\n",
        "text = \" \".join(review for review in data_pos.text)\n",
        "print (\"There are {} words in the combination of all positive reviews.\".format(len(text))) \n",
        "plt.figure(figsize=(10,7))\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopword,background_color=\"white\").generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eMUnXkUohJ34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_neg = data[data['label'] == 0]\n",
        "text = \" \".join(review for review in data_neg.text)\n",
        "print (\"There are {} words in the combination of all negative reviews.\".format(len(text))) \n",
        "plt.figure(figsize=(10,7))\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopword,background_color=\"white\").generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "unPHH4-ihMAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf = CountVectorizer(ngram_range=(1,1))  \n",
        "tf_idf.fit(data['text']) \n",
        "print(\"Printing Some Features: \", tf_idf.get_feature_names()[0:10]) \n",
        "final_tfidf = tf_idf.transform(data['text']) \n",
        "print('The type of tfidf matrix: ', type(final_tfidf)) \n",
        "print('The shape of tfidf matrix: ',final_tfidf.get_shape()) \n",
        "print('The number of unique values: ',final_tfidf.get_shape()[1])"
      ],
      "metadata": {
        "id": "0zpxXqOShO1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=data.label.values #converting the label column into an array\n",
        "x=final_tfidf\n"
      ],
      "metadata": {
        "id": "w52W3DUFhRIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "CIlK2Gy8hRAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "X_cv,X_test,y_cv,y_test=train_test_split(X_train,y_train,test_size=0.2)\n",
        "print(\"The shape of X_train: \",X_train.shape) \n",
        "print(\"The shape of y_train: \",y_train.shape) \n",
        "print(\"The shape of X_test: \",X_test.shape) \n",
        "print(\"The shape of y_test: \",y_test.shape) "
      ],
      "metadata": {
        "id": "flUtDPdHhTkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=DecisionTreeClassifier(criterion='gini').fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "sfAq_wXMh9Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy on Decision Tree:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "cJ8H8cB_iAWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "sTJtdLO3iDg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "bq1z1H0LiFef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "FoZvhuMPiIth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy on KNN:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "r3H5v3-oiJUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "xxV2eZYXiLCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "RmXxMQTfiO3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=RandomForestClassifier(n_estimators=600,n_jobs=-1,max_depth=2).fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "J7apLNyDiQeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy on RFC:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "lw-wKO9ViSLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "CG4StnGoiUI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "o7zWRgmTiWOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc=GradientBoostingClassifier(n_estimators=200,learning_rate=0.1,max_depth=2) \n",
        "gbc.fit(X_train,y_train)\n",
        "ypred = gbc.predict(X_test) "
      ],
      "metadata": {
        "id": "C8aXYZC7iYS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy on GradientBoostingClassifier: \",metrics.accuracy_score(y_test, ypred))"
      ],
      "metadata": {
        "id": "gK1r-bjuiZIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,ypred))"
      ],
      "metadata": {
        "id": "9lr7MT45ickD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,ypred))"
      ],
      "metadata": {
        "id": "boUuKJXEieb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNB = MultinomialNB().fit(X_train,y_train)\n",
        "y_pred = MNB.predict(X_test)\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy on Naive_Bayes:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "JpAC7CZAiocO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "RBSXiUMsitvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "XCdsD-73ivic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=LogisticRegressionCV(cv=10,scoring='accuracy',n_jobs=-1,verbose=1,max_iter=500)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "3hMPdL2viwhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "params = clf.get_params()\n",
        "params"
      ],
      "metadata": {
        "id": "uqdT6T66izhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf2 = LogisticRegression(dual=False,max_iter=500,n_jobs=-1,penalty='l2',solver='lbfgs',tol=0.0001,verbose=1)\n",
        "clf2.fit(X_train,y_train)\n",
        "y_pred = clf2.predict(X_test)"
      ],
      "metadata": {
        "id": "o7qgHUjti21b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy on Logistic Regression: \",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "P8RyLzd_i4mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(clf2,open('/content/LR movie new.pkl','wb'))"
      ],
      "metadata": {
        "id": "kD7R_-yvi7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "DiSqhWYVi9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm=confusion_matrix(y_test,y_pred)"
      ],
      "metadata": {
        "id": "ojJWmqvQi_nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf() \n",
        "plt.imshow(cm,interpolation='nearest',cmap='coolwarm') \n",
        "classnames = ['Negative','Positive'] \n",
        "tick_marks = np.arange(len(classnames)) \n",
        "plt.ylabel('True Label') \n",
        "plt.xlabel('Predicted label') \n",
        "plt.title('Positive or Negative Sentiment- Confusion matrix')\n",
        "plt.xticks(tick_marks,classnames,rotation=45) \n",
        "plt.yticks(tick_marks,classnames) \n",
        "S = [['TN',\"FP\"],['FN','TP']] \n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j,i,str(S[i][j])+\" = \"+str(cm[i][j])) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vw7VwpqgjB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(clf,open('/content/drive/MyDrive/LR movie.pkl','wb'))\n",
        "pickle.dump(tf_idf,open('/content/drive/MyDrive/tf_idf movie.pkl','wb'))"
      ],
      "metadata": {
        "id": "XxvoTVyOjEYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "wd = \"/content/\"\n",
        "with open(wd + \"filename.obj\",\"wb\") as f:\n",
        "    dill.dump(clf,f)\n"
      ],
      "metadata": {
        "id": "nCkcBhenjFIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.coef_"
      ],
      "metadata": {
        "id": "hvtRfhgRjIEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [\"I cannot phrase it better, so I will quote Rex Reed who called Inception's storyline prattling drivel. A friend claimed Inception is a thinking person's movie, but a thinking person will realize that it is only masquerading as a thinking person's movie. At bottom, intellectually, there is no there there. Add to this that someone clearly believed the film needed to be pumped up with overwrought drama to qualify it as a summer blockbuster. I couldn't wait for it to end, and when it did, the intrusiveness of the loud, schlocky music over the closing credits seemed to crystallize all the incipient negative feelings I had been having throughout the movie. I hope that this director will go back to doing smaller films that do not stretch his concepts beyond what they can support.\"]\n",
        "a =tf_idf.transform(a).toarray()"
      ],
      "metadata": {
        "id": "U2X6XnEnjIta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNB.predict(a)"
      ],
      "metadata": {
        "id": "AaYD6llCjOpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "m = np.array([2]) \n",
        "type(m[0]) "
      ],
      "metadata": {
        "id": "uBHS9doJjQzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: After implementing various models, We arrive at the conclusion that the following three are giving the best results:\n",
        "1)Logistic Regression--> Accuracy_Score : 88.59%\n",
        "2)Multinomial Naive Bayes--> Accuracy_Score: 85.73%"
      ],
      "metadata": {
        "id": "3n98Us3MjTDa"
      }
    }
  ]
}